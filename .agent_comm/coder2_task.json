{
  "agent_id": "coder2",
  "task_id": "task_6",
  "files": [
    {
      "name": "augmentation.py",
      "purpose": "Data augmentation techniques",
      "priority": "medium"
    },
    {
      "name": "feature_extraction.py",
      "purpose": "Feature extraction layers",
      "priority": "medium"
    }
  ],
  "project_info": {
    "project_name": "enhanced_cs.NE_2508.08877v1_Towards_Scalable_Lottery_Ticket_Networks_using_Gen",
    "project_type": "computer_vision",
    "description": "Enhanced AI project based on cs.NE_2508.08877v1_Towards-Scalable-Lottery-Ticket-Networks-using-Gen with content analysis. Detected project type: computer vision (confidence score: 8 matches).",
    "key_algorithms": [
      "Demanding",
      "Following",
      "Edge-Popup",
      "Each",
      "Smaller",
      "Analyzed",
      "Rate",
      "Reinforcement",
      "Visualization",
      "Selection"
    ],
    "main_libraries": [
      "torch",
      "numpy",
      "pandas"
    ]
  },
  "paper_content": "PDF: cs.NE_2508.08877v1_Towards-Scalable-Lottery-Ticket-Networks-using-Gen.pdf\nChunk: 1/1\n==================================================\n\n--- Page 1 ---\nTowards Scalable Lottery Ticket Networks using\nGenetic Algorithms\nJulian Sch\u00f6nberger, Maximilian Zorn, Jonas N\u00fc\u00dflein, Thomas Gabor, and\nPhilipp Altmann\nLMU Munich, Munich, Germany\njulian.schoenberger@ifi.lmu.de\nAbstract. Building modern deep learning systems that are not just effective but\nalso efficient requires rethinking established paradigms for model training and\nneural architecture design. Instead of adapting highly overparameterized net-\nworks and subsequently applying model compression techniques to reduce re-\nsource consumption, a new class of high-performing networks skips the need for\nexpensive parameter updates, while requiring only a fraction of parameters, mak-\ning them highly scalable. The Strong Lottery Ticket Hypothesis posits that within\nrandomly initialized, sufficiently overparameterized neural networks, there exist\nsubnetworks that can match the accuracy of the trained original model\u2014without\nany training. This work explores the usage of genetic algorithms for identify-\ning these strong lottery ticket subnetworks. We find that for instances of binary\nand multi-class classification tasks, our approach achieves better accuracies and\nsparsity levels than the current state-of-the-art without requiring any gradient in-\nformation. In addition, we provide justification for the need for appropriate eval-\nuation metrics when scaling to more complex network architectures and learning\ntasks.\nKeywords: Strong Lottery Ticket Hypothesis \u00b7 Evolutionary Optimization \u00b7 Neu-\nroevolution \u00b7 Neural Architecture Search \u00b7 Loss Landscape Analysis \u00b7 Pruning.\nAn earlier version of this work was presented at the International Conference on Neural\nComputation Theory and Applications (NCTA 2024) [1]. This article extends our conference\npaper with updates to the method to improve multi-class classification, an expanded experimental\nsetup, and a multi-class performance anaylsis with visual and analytical justifications.arXiv:2508.08877v1  [cs.LG]  12 Aug 2025\n\n--- Page 2 ---\n2 J. Sch\u00f6nberger et al.\n1 Introduction\nModern deep learning models consist of billions of parameters, that require up to mil-\nlions of optimization steps to train, taking weeks or even months on high-performance\ncomputing hardware. The consequences include, but are not limited to, high economic\nexpenses and excessive energy consumption, often leading to a substantial amount of\nCO2emissions. Future Deep Learning models need to be more efficient and effective\nto be sustainable in the long-term.\nWe argue that the massive overparameterization of today\u2019s models, which is one major\ncontributor to the success of deep learning is also a significant source of inefficiency,\nbut may inadvertently provide a path to a new class of highly scalable model archi-\ntectures. This overparameterization allows for the existence of strong lottery tickets\n\u2014 subnetworks within the randomly initialized larger model that achieve comparable\nperformance to the full network without any training [22], an example of which is il-\nlustrated in Fig. 1. By simply pruning parameters (i.e., setting their values to zero), we\ncan shift the model to an optimal region in the parameter space, without having to per-\nform small-scale updates to the entire network parameterization, thereby reducing the\ncomputational cost and potentially improving efficiency. This alternative paradigm to\nclassical training not only has the potential to reduce the number of optimization steps\nrequired to achieve a comparable or better performance, but also leads to a reduction in\nmodel complexity (w.r.t. the model size).\nThe efficacy of this approach can be further enhanced by using a sophisticated optimiza-\ntion algorithm that fully leverages the new discrete nature of the optimization problem.\nIn this work, we argue for the benefit of using genetic algorithms (GAs) as capable\ncombinatorial optimization methods that can explore large areas of the complex solu-\ntion space. In contrast to gradient-based methods, genetic algorithms are architecture-\nagnostic, meaning they can be applied to any artificial neural network (ANN) architec-\nture regardless of its structure or layer types. This flexibility enables searching for sub-\nnetworks in a diverse range of models, from simple feedforward networks to large scale\ntransformer architectures. Furthermore, since these algorithms do not rely on gradient\ninformation, they can be applied to non-differentiable models or novel architectures\nwhere gradient-based methods might not be directly applicable.\nFig. 1: Visualization of a lottery ticket network [1]. Left: Full network graph, where red\nconnections are those that consistently appear in most evolved lottery ticket networks\nwithin a sample population, while blue connections do not. Right : An example of an\nevolved lottery ticket subnetwork, retaining only a subset of active connections.\n\n--- Page 3 ---\nTowards Scalable Lottery Ticket Networks using Genetic Algorithms 3\nThis work presents an extensive analysis of the significance of using genetic algorithms\nfor identifying strong lottery tickets in randomly initialized feed-forward neural net-\nworks. It forms the basis for future research on more complex model architectures.\nAfter explaining the major components of our algorithm, we perform a comprehensive\nperformance evaluation on two binary as well as two multi-class classification tasks.\nWe complement our findings with the comparative performance of the state-of-the-art\nas well as networks obtained through standard training. We extend a previous version\nof this paper (cf. [1]) by providing empirical and analytical justifications for the use of\na different performance criterion in solving the multi-class classification tasks, as well\nas detailed evaluations of said tasks. In addition, we incorporate a \u201cpost-evolutionary\npruning\u201d routine that further improves the accuracy-sparsity trade-off present during\noptimization.\nOur main contributions include the successful application of a genetic algorithm to\nidentify strong lottery ticket networks for binary and multi-class classification tasks,\nsurpassing state-of-the-art accuracies and sparsities in several instances. Additionally,\nwe provide both visual and analytical insights into the inherent properties of strong\nlottery tickets.\n2 Related Work\nThe Lottery Ticket Hypothesis has attracted significant attention in recent years, leading\nto the identification of various connections to related fields. In this section, we will\nexplore the existing literature and discuss its relevance to our work.\nLottery Ticket Hypothesis Frankle and Carbin [6] discovered that a network, after be-\ning pruned and having its remaining weights reset to their original random initializa-\ntion, could be retrained to achieve test accuracy comparable to the original network in a\nsimilar number of training iterations. They termed this phenomenon the Lottery Ticket\nHypothesis (LTH) and referred to the pruned subnetwork as a winning ticket . They pro-\nposed an algorithm based on iterative magnitude pruning to identify these winning tick-\nets. Since then, numerous methods have been developed to identify these subnetworks.\nFor instance, Jackson et al. [11] employed an evolutionary algorithm where fitness is\ncalculated based on network density and validation loss, addressing the trade-off be-\ntween subnetwork sparsity and accuracy. Other subsequent studies [27, 34] extended\nthe LTH, demonstrating that it is possible to find subnetworks within randomly initial-\nized networks that perform better than random guessing, even without training. Zhou\net al. [34] introduced neural network masking as an alternative training method and\nintroduced the concept of \u201csupermasks.\u201d\nStrong Lottery Ticket Hypothesis Ramanujan et al. [22] expanded on this concept by\nintroducing the Strong Lottery Ticket Hypothesis (SLTH), which posits that a suffi-\nciently overparameterized neural network with random initialization contains a subnet-\nwork, called strong lottery ticket (SLT), that can achieve competitive accuracy (relative\nto the large, trained network) without requiring any training [17]. In addition, they pro-\nposed the edge-popup algorithm as a method for identifying strong lottery tickets. It\n\n--- Page 4 ---\n4 J. Sch\u00f6nberger et al.\nutilizes a gradient estimation technique to approximate the gradient of a pop-up score\nfor each network weight. These pop-up scores are updated through stochastic gradient\ndescent (SGD). A series of theoretical works explored the necessary degree of over-\nparameterization [17, 18, 21], demonstrating that logarithmic overparameterization is\nsufficient [18, 21]. In their pursuit of more efficient methods for discovering SLTs,\nWhitaker [29] proposed three theoretical quantum algorithms based on edge-popup,\nknowledge distillation [9], and NK Echo State Networks [30]. Finally, Chen et al. [3]\nintroduced a new category of high-performing subnetworks called \u201cdisguised subnet-\nworks\u201d. Unlike regular SLTs, these subnetworks must first be \u201cunmasked\u201d through spe-\ncific weight transformations. The authors identify these unique subnetworks using a\ntwo-step algorithm that applies sign flips to the weights of pruned networks, leverag-\ning Synflow [24]. By leveraging these additional weight transformations, they obtain\nsubnetworks that have greater capacity than regular SLTs.\nWeak Lottery Ticket Hypothesis A limited number of methods for discovering strong\nlottery tickets have been developed so far, with most empirical research focusing on the\noriginal Lottery Ticket Hypothesis. These methods identify so-called weak lottery tick-\nets, which can achieve competitive accuracies on much smaller subnetworks, but only\nafter re-training the subnetworks\u2019 weights. This training-pruning-retraining cycle is of-\nten resource-intensive, and the advantages over standard training are not always clear.\nConversely, the search for strong lottery tickets enables the discovery of high-accuracy\nsubnetworks without the need for expensive retraining steps. Moreover, when integrated\nwith meta-heuristic optimization, this methodology can be employed in the context of\nstructures exhibiting discontinuous functions, a domain in which gradient-based meth-\nods encounter significant challenges. In this paper, we propose a novel approach for\nidentifying strong lottery tickets, based on genetic algorithms, leveraging the principles\nof biological evolution. In contrast to conventional methods that often employ heuristics\nand pseudo-training algorithms with gradient descent and pre-defined pruning rates, our\napproach does not necessitate gradient information. It enables direct optimization of the\nsubnetwork structure, without imposing artificial constraints on the maximum number\nof pruned weights.\nGenetic algorithms have been shown to be highly effective in the resolution of NP-hard\nproblems and are particularly well-suited for the optimization of non-convex objective\nfunctions with numerous local minima, saddle points, and plateaus. The optimization\nlandscape of the Strong Lottery Ticket Hypothesis is complex, influenced by factors\nsuch as masking, random initialization, the scale of the feature space and the objective\nfunction. It is worth noting that Jackson et al. [11], while using an approach similar to\nours, apply their evolutionary algorithm to the original LTH and thus can only identify\nweak lottery tickets that require retraining.\nExtreme Learning Machine Huang et al. [10] introduced the Extreme Learning Ma-\nchine (ELM), a method conceptually similar to the Strong Lottery Ticket Hypothesis,\nwhere the random parameter values of the hidden layer in a single-hidden-layer neural\nnetwork are fixed. The optimal weights for the output layer are then computed using\nthe closed-form solution for linear regression. In contrast to SLTs, dense models such\nas ELM are less parameter-efficient, face challenges in scaling to deeper architectures\n\n--- Page 5 ---\nTowards Scalable Lottery Ticket Networks using Genetic Algorithms 5\n(which often require complex adaptations, such as those based on autoencoders [12]),\nand involve the computation of matrix inverses, which is computationally expensive.\nNeural Architecture Search The process of neural architecture search (NAS) shares\ncertain similarities with the search for lottery tickets, given that both involve the genera-\ntion of networks with previously unknown structures and untrained (though potentially\nselected) weights. Gaier and Ha [7] examined the impact of network architecture ver-\nsus parameter initialization on task performance. Upon initializing all parameters with\na single value drawn from a uniform distribution, they discovered that certain architec-\ntures attain higher accuracy than random on the MNIST dataset. In their seminal work,\nWortsman et al. [31] proposed a method that facilitates the continuous adaptation of a\nnetwork\u2019s connection graph and its associated parameters during the training process.\nTheir innovative approach demonstrated that the resulting networks exhibited superior\nperformance in comparison to both manually designed and randomly structured net-\nworks. In contrast to our approach, Gaier and Ha [7] used a single fixed value for the\nparameters, rather than sampling from a random distribution. The method introduced\nby Wortsman et al. [31] offers an alternative to finding winning tickets. Ramanujan et\nal. [22] later introduced edge-popup, inspired by Wortsman et al.\u2019s work, but since their\napproach involves learning the network structure and its parameterization together, it\ncannot be applied for finding pruning masks for strong lottery tickets.\nEvolutionary Pruning Unlike NAS or neuroevolution, which typically involve evolving\nthe network\u2019s topology, evolutionary pruning focuses solely on removing connections\nand possibly entire neurons from the network graph. With these techniques, networks\ncan often be reduced in size without sacrificing performance. This area of research\nincludes methods that differ in their choice of solution representation (direct or in-\ndirect encoding) and the number of objectives considered. Direct encoding methods\nfrequently utilize binary masks that are applied to network structures, such as individ-\nual weights or convolution filters [32]. Common multi-objective tasks include not only\nsparsity but also accuracy improvement or energy efficiency [28]. Our approach em-\nploys binary pruning masks with two objectives in mind: accuracy and sparsity. To the\nbest of our knowledge, we are the first to apply evolutionary pruning within the context\nof the SLTH.\nOther Pruning Methods As noted by Wang et al. [26], in addition to the classic LTH,\nwhich uses static pruning masks on trained networks, and the SLTH, which bypasses\ntraining entirely, there exists a third category of methods that prune networks at initial-\nization using pre-selected masks [13, 24, 25]. For instance, Lee et al. [13] proposed a\npruning mask created before training, which removed structurally insignificant connec-\ntions based on a new saliency criterion known as connection sensitivity. Similar to our\napproach, their method is one-shot, as the network only requires a single pruning step;\nhowever, training is still involved, and specific pruning criteria are needed to identify\noptimal subnetworks.\n\n--- Page 6 ---\n6 J. Sch\u00f6nberger et al.\n3 Method\nThe components of the genetic algorithm are examined in detail below. This exam-\nination includes the configuration of candidate solutions, the evaluation process, the\nselection of parents and survivors and the series of genetic operations that govern the\nevolutionary process.\nSolution Representation The generation of strong lottery ticket networks is achieved\nby implementing an evolutionary algorithm, assuming that the intended task of the net-\nwork is predetermined, (e.g., specified by an objective function such as a classification\naccuracy function or a loss function, denoted by F). The architecture graph of the com-\nplete network and the vector of its randomly initialized parameters, w=\u27e8w0, ..., w n\u27e9\nwithwi\u2208Rfor all i, are also provided. The proposed approach then generates a\nbit mask (genotype), denoted by b=\u27e8b1, ..., b n\u27e9with bi\u2208 {0,1}for all i. This\nmask is used to construct a masked network (subnetwork), or phenotype, represented\nbyw\u2032=\u27e8bi\u00b7wi\u27e9i=1,...,n. These subnetworks are typically considerably smaller than\nthe original networks w.r.t. non-zero weights, without significant performance degra-\ndation w.r.t. F. In formal terms, let w\u2217denote the nweights of the trained full net-\nwork, then it follows that bshould satisfy the following equation:Pn\ni=0bi< < n and\nF(w\u2032)\u2248 F(w\u2217). It is important to note that the following discourse refers exclusively\nto the weights within the parameter vector. It does not encompass any potential bias\nnodes. Even though they are not pruned, biases are still initialized the same way as the\nweights.\nFitness and Selection In order to promote the development of strong lottery tickets,\na lexical evolutionary optimization process is employed. Two objectives are pursued:\nWe primarily focus on identifying subnetworks that attain the level of accuracy accom-\nplished through conventional training; subsequently, we also promote the identification\nof subnetworks that are as sparse as possible without compromising accuracy. This\nmulti-objective approach enables the pruning of subnetworks to a considerable extent,\neven in scenarios where high accuracies have already been attained. An intriguing char-\nacteristic of using an evolutionary optimization approach compared to gradient-based\noptimization is that we can directly optimize the accuracy function without having to\nrely on the loss function intermediary (i.e. F=A). Our later findings prove that this\napproach can be very effective for binary classification problems, but appears to reach\nits limits when the number of classes increases (cf. 5.3). Fortunately, our primary per-\nformance metric can be exchanged seamlessly, such that contrary to maximizing the\naccuracy function we can e.g. minimize a loss function instead (i.e. F=L). The as-\nsessment of individuals within the evolutionary framework takes place in two process-\ning steps: In the first step, the objective is to identify parents (i.e., individuals suitable\nfor recombination). In this stage, we solely focus on the performance goal. Conversely,\nin the second step, the objective is to select survivors (i.e., individuals suitable for the\nnext generation). In this stage, we also consider the sparsity objective. This differenti-\nation reflects the observation that recombination is the main contributor for producing\nhigher-performing individuals over the course of the evolution. Focusing on the per-\nformance goal (i.e. accuracy/loss) in parent selection leads to an effective prioritization\n\n--- Page 7 ---\nTowards Scalable Lottery Ticket Networks using Genetic Algorithms 7\nstrategy: the fitness of individuals is determined by the measured performance on the\ntraining dataset, and individuals are then ranked accordingly. Although sparsity is con-\nsidered in survivor selection, performance remains the primary factor: Within groups of\nindividuals with the same performance level, those with higher sparsity are preferred.\nWe employ an (elitist) cut-off selection method for the purpose of survivor selection.1\nThis method entails the selection of the top kindividuals from the current population\nand their subsequent transfer to the following generation\u2019s population. For our GA it\nholds that k=Nwhere Ndenotes the original population size.\nIt was observed that, due to the fact that our defined genetic operators don\u2019t operate \u201cin\nplace\u201d, the population frequently surpassed its initial size Nduring generation transi-\ntions, which necessitated the reduction of the population size for the subsequent gen-\neration. Concerning the process of parent selection, any individual has the capacity to\nbe selected as a first parent, with probability rec_rate \u2208[0,1], and subsequently\npaired with a second parent, selected at random from the top lindividuals in the present\npopulation, where l=N\u00b7par_rate where par_rate is another hyperparameter.\nGenetic Operators The initial population is generated in two steps. First, individuals\nare uniformly generated at random, meaning that each bit has an equal chance of being\nselected at any position in the binary pruning mask. Second, from the individuals that\nwere randomly generated, those that don\u2019t reach a certain accuracy bound are discarded.\nIn our implementation, an adaptive bound undergoes dynamic reduction if an insuffi-\ncient number of individuals fulfill the specified boundary value in a given time frame.\nThis process follows the outline of a predefined exponential function. This is done to\nlessen the impact of random sampling on runtime. The employment of an adaptive ac-\ncuracy bound enables the establishment of an initial bound that is higher compared to\nwhat is tolerable with a static bound, and it has been demonstrated to result in superior\nfinal accuracies. In the subsequent discussion, the configuration that executes solely the\ninitial step we will refer to as GA, and the configuration where an adaptive accuracy\nboundary is used (i.e. the first and second step) we will call GA (adaptive AB) .2\nWe implement a single-point mutation process, wherein individuals are randomly se-\nlected from the current population at a rate denoted by mut_rate and a mutant is\ngenerated through a random bit flip. For recombination, we employ a random crossover\nmechanism between two parents. It is noteworthy that mutants and offspring are directly\nincorporated into the population, with no direct replacement of their source individuals.\nIn each generation, we introduce mnewly generated individuals to the population to\nfurther enhance diversity within the population. The value of m=N\u00b7mig_rate is\ndefined by the hyperparameter mig_rate .\n1Alternative selection methods, including roulette and random walk selection, were also ex-\nperimented with; however, it was found that the selection method employed did not have a sig-\nnificant impact.\n2Code for the GA and our experiments is available at https://github.com/\njulianscher/SLTN-GA .\n\n--- Page 8 ---\n8 J. Sch\u00f6nberger et al.\nPost-Evolutionary Pruning Since our fitness evaluation and selection criteria prioritize\nthe performance objective, we observe that after the GA has terminated, the possibilities\nfor pruning some of the remaining connections are typically not yet fully exhausted.\nThat\u2019s why we employ an additional simple post-processing routine that sequentially\nruns through the final bit mask, setting all those bits to zero that do not exert a negative\ninfluence on the accuracy. Depending on factors like the initial sparsity, the prune-rate\nused to create the individuals for the initial population and the duration of the evolution,\nthe post-processing routine has demonstrated to enhance the sparsity of the final indi-\nviduals by a margin of multiple percentage points and can even lead to minor additional\naccuracy improvements (cf. Fig. 11).\n4 Experimental Setup\nTo evaluate the performance of the above genetic algorithm in finding SLTs, we apply\nit to several datasets of varying complexity and different network architectures with an\nincreasing number of parameters. In addition to binary classification problems, we also\ninclude two multi-class classification tasks. To complement our GA results, we com-\npare the performance with the state of the art and with networks trained with standard\nbackpropagation.\nHyperparameters In the subsequent series of experiments, the genetic algorithm is im-\nplemented with a fixed population size of N= 100 individuals, while the rates for\nparent selection, recombination, mutation, and migration remain constant. For the pro-\ncess of recombination, a value of 0.3 is employed, which dictates that approximately\n30% of the total population is selected to become a first parent. For the purpose of gen-\nerating offspring, a recombination partner is randomly selected from the top 30% of\nthe population due to the recombination rate of rec_rate = 0.3. The mutation rate,\nset at mute_rate = 0.1, ensures that approximately 10% of individuals generate a\nmutant that is added to the population, which is a relatively high value. This is done\nwith the intention of generating highly exploratory runs. The migration rate, defined as\nmig_rate = 0.1, is set to ensure that approximately 10% of the individuals in the\ninterim population prior to survivor selection are newly generated. Table 1 provides a\nsummary of the selected hyperparameter values. The termination of the GA requires the\nevolution of the population for a minimum of 100 generations and activates when there\nis no discernible improvement in performance for the past 50generations. When em-\nploying the GA (adaptive AB), we limit the evolution to a maximum of 200 generations,\nas we have observed that beyond this point, the improvement in performance is typically\nnegligible. An explicit hyperparameter search was not conducted to determine optimal\nvalues; instead, decisions were based on observations made during the implementation\nphase. This approach deliberately focuses on the expected performance rather than the\nhypothetically maximal one.\nDatasets The experiments were conducted on the basis of four different data sets, with\ndifferent degrees and sources of complexity. Classification problems were selected for\nthe experiments as they are characterized by their interpretability and the availability of\n\n--- Page 9 ---\nTowards Scalable Lottery Ticket Networks using Genetic Algorithms 9\nHyperparameter Value\npop_size N 100\nrec_rate 0.3\npar_rate 0.3\nmut_rate 0.1\nmig_rate 0.1\nTable 1: The hyperparameters utilized for our GA evaluation. Table taken from [1].\na well-established evaluation metric that facilitates objective analysis. As illustrated in\nFig. 2a, the two-dimensional moons dataset comprises two moon-shaped point clus-\nters with minimal overlap, classified into only two categories. A neural network with\none hidden layer consisting of 6 hidden units, and trained via backpropagation, attains\napproximately 100% accuracy in select runs.\nContrasting with this relatively simple dataset, we have chosen the more complex 2D\nbinary classification problem represented by the circles dataset, as shown in Fig. 2b.\nThis data set comprises two different classes that form two concentric circles. The larger\nring is arranged in such a way that it encloses the smaller ring. The transition from the\nsmaller to the larger ring is instantaneous, and there are numerous points in the overlap\nregion. This property poses a significant challenge even for the trained dense network\nand emphasizes the complexity of the problem at hand. A total of 66,000random data\npoints were generated for both datasets. Subsequently, Gaussian noise with a standard\ndeviation of 0.07was added to each dataset. To test the GA\u2019s performance in multi-class\nclassification scenarios, we artificially generated the blobs dataset, which comprises\nup to 10 distinct two-dimensional Gaussian-shaped clusters, being assigned class labels\n{0, ...,9}. These clusters are uniformly dispersed in the feature space, ensuring they\ndo not overlap, as depicted in Fig. 2c. Networks optimized with the backpropagation\nalgorithm have been observed to achieve 100% accuracy in the classification of points\nwithin the two-dimensional space, irrespective of the number of distinct classes (cf.\nFig. 9). As a final benchmark, we included the digits dataset (cf. Fig. 2d), which\nis a lower-dimensional version similar to the popular MNIST dataset for classifying\nhandwritten digits [4]. It consists of 1797 images with 8\u00d78input features correspond-\ning to individual pixels, and are assigned class labels {0, ...,9}. The datasets were split\ninto a training set and a test set, with 25% of the data points designated for evaluation.\nFurthermore, to circumvent the potential adverse scaling effects that might originate\nfrom non-Gaussian distributions, a min-max normalization procedure was applied to\nthemoons and the digits datasets.\nNetwork Architectures We employ feed-forward ANNs with rectified linear unit activa-\ntion functions (ReLU) exclusively for the neurons in the input and hidden layers. When\nwe use accuracy as performance objective, we do not incorporate a softmax activation\nfunction, since we can directly compute accuracies without requiring class probabilities,\nby determining the class of the network output with the highest value. To gain insight\ninto the behavior of the GA across various model sizes, four network architectures are\n\n--- Page 10 ---\n10 J. Sch\u00f6nberger et al.\n(a) Moons dataset\n (b) Circles dataset\n(c) Blobs dataset\n (d) Digits dataset\nArchitecture moons ,circles blobs digits\nA [2, 20, 2] [2, 20, n] [64, 20, n]\nB [2, 75, 2] [2, 75, n] [64, 75, n]\nC [2, 100, 2] [2, 100, n] [64, 100, n]\nD [2, 50, 50, 2] [2, 50, 50, n] [64, 50, 50, n]\n(e) Network Architectures\nFig. 2: Overview of the datasets and network architectures used in our study: (a) The\nmoons test dataset, comprising 16,000 two-dimensional data points normalized within\nthe range [\u22120.7,0.7], and (b) the circles test dataset, featuring two concentric rings\nwith 16,000 two-dimensional data points. (c) We generated 10 randomly distributed\nclusters consisting of 12,500 data points for the two-dimensional blobs test dataset\nand (d) shows a random subset of samples from the digits dataset. All datasets have\nbeen generated using methods provided by scikit-learn [20]. The network archi-\ntectures (e) are identified by single-letter codes. The bracket notation represents the\nnumber of neurons in each layer, where the first value indicates the number of input\nneurons, and the last value denotes the number of output neurons. For the blobs and\ndigits dataset, the number of output neurons ndepends on the number of classes\nused. Figures and Table adapted from [1].\n\n--- Page 11 ---\nTowards Scalable Lottery Ticket Networks using Genetic Algorithms 11\nexamined in the experimental section (Table 2e). To streamline the presentation of the\nsubsequent plots, the analyzed network architectures are denoted by \u201cA\u201d, \u201cB\u201d, \u201cC\u201d, and\n\u201cD\u201d for convenience. Our studies demonstrated that the selection of the network pa-\nrameter initialization method has a substantial influence on the final accuracy achieved.\nIn our experiments using the genetic algorithm, we uniformly sampled the network\nweights from the interval [\u22121,1]. This approach consistently yielded the most optimal\nresults across all examined datasets. These findings are supported by theoretical proofs\nfor the existence of SLTs in uniformly initialized networks [17, 21]. Notably, while the\nmajority of research in the area of SLTH employs zero biases, our experiments revealed\na substantial improvement in performance when biases were initialized based on the\nsame uniform distribution.\nBaselines Since strong lottery tickets are defined based on their comparative perfor-\nmance to networks obtained by classical gradient-based training, we use backprop-\nagation as a baseline. To benchmark against a well-optimized implementation of a\nfeed-forward network and its trainer, we employed scikit-learn\u2019s MLPClassifier and\nconducted hyperparameter tuning across all 4architectures using their Randomized-\nSearchCV method. We choose random search for its computational efficiency in nav-\nigating large parameter spaces with limited resources. The parameter ranges were de-\nfined based on prior insights and preliminary trials. Tuned hyperparameters include\nsolvers, learning rates, batch sizes, momentum, L2 regularization strengths (alphas),\nand epsilon values for numerical stability. Table 2 summarizes the selected values. To\nensure convergence, both the search and training ran for 1000 epochs. Our study evalu-\nates the mean test accuracies of backpropagation-trained networks listed in Table 2e.\nDataset Classes Solver Learning Rate Learning Rate Init Epsilon Batch Size Alpha Momentum\nmoons (A-D)2 adam constant 0.021544 4.64e-09 128 0.0001 -\n2 adam constant 0.001 4.64e-09 64 0.000215 -\n2 adam constant 0.001 4.64e-09 64 0.000215 -\n2 adam constant 0.001 4.64e-09 64 0.000215 -\ncircles (A-D)2 sgd adaptive 0.1 - 64 0.046416 0.0\n2 sgd adaptive 0.004642 - 128 0.046416 0.5, nesterov\n2 adam constant 0.001 4.64e-09 64 0.000215 -\n2 sgd adaptive 0.1 - 128 0.046416 0.0, nesterov\nblobs (C) 2-10 adam constant 0.001 4.64e-09 64 0.000215 -\ndigits (B)2 adam constant 0.001 4.64e-09 64 0.000215 -\n3 adam constant 0.021544 1.67e-09 128 0.046416 -\n4 adam adaptive 0.002783 7.74e-09 32 0.004642 -\n5 adam adaptive 0.002783 5.99e-08 32 0.01 -\n10 adam adaptive 0.002783 7.74e-09 32 0.004642 -\nTable 2: Listing of the determined backpropagation hyperparameters for the MLPClas-\nsifier model from scikit-learn using random search. The letters in brackets correspond\nto our 4 network architectures. For the two multi-class datasets, we only consider two\narchitectures in our experiments. Table adapted from [1].\n\n--- Page 12 ---\n12 J. Sch\u00f6nberger et al.\n5 Experimental Results\nThis section provides a comprehensive evaluation of the GA\u2019s capabilities in finding\nstrong lottery tickets. Section 5.1 investigates the average performance of our two GA\nconfigurations on the moons andcircles datasets, followed by a general analysis\nregarding runtime complexity. Section 5.2 contains an ablation study for the state-of-\nthe-art edge-popup, where we analyze the impact of different weight initializations on\nthe algorithms\u2019 performance. Based on our results, we conduct a broad performance\nstudy in which we compare the accuracy of the subnets found by our GA with that\nfound by edge-popup and the backpropagation-trained networks. We employ a mixed\nlinear model, to quantify the statistical significance of our results. Finally, in Section 5.3,\nwe test our best GA configuration on our multi-class classification tasks, using accu-\nracy and cross-entropy loss as performance metrics for evaluating our individuals. We\nanalyze how the choice of the performance objective affects the complexity of the op-\ntimization, as well as how important the normalization of datasets is. We conclude by\nhighlighting the additional sparsity gains we achieve by using the post-evolutionary\npruning routine.\n5.1 GA Performance Analysis\nAs previously stated, our experiments utilize four distinct network architectures (see Ta-\nble 2e). We hypothesize that networks with more parameters are more likely to include\nfavorable initializations, potentially yielding higher-performing subnetworks. Further-\nmore, we investigate whether applying an accuracy abound when generating the initial\npopulation influences the subsequent evolutionary process. In this section, we only con-\nsider binary classification problems, and we use accuracy as the performance objective\nwhen evaluating the fitness of individuals.\nAs illustrated in Fig. 3a, the outcomes of the moons dataset indicate that the genetic\nalgorithm consistently achieves high final accuracies for network D, attaining a mean\naccuracy approaching 100%. Upon examining the variability among GA runs across\ndifferent network architectures, a noticeable correlation emerges between the quantity\nof network parameters and the resulting performance. For the least complex network\nA, which contains only 80 parameters, the mean difference to backpropagation is ap-\nproximately 9%. As the parameter count increases, the discrepancy between the two\nmethods decreases steadily. For networks C and D, the mean value approaches that of\nbackpropagation, and for network D, there is minimal variance between the runs.There\nis less variability in performance between the distinct GA configurations. Overall, the\nmean for the runs using an accuracy bound seem to exhibit slight increases relative to\nthe mean from runs not employing an accuracy bound, though this effect diminishes\nwith increasing network sizes.\nThe results shown in Fig. 3b are largely consistent with the previous findings. A com-\nparison of the mean performance of backpropagation across the two datasets reveals\nhigher complexity levels for the circles dataset. The GA consistently achieves the\nlowest accuracies on network architecture A, but achieves higher final accuracies on\nlarger networks. The usage of network D resulted in the highest mean accuracy, with\na result of 91.6%. Notably, this outcome was attained without the employment of an\n\n--- Page 13 ---\nTowards Scalable Lottery Ticket Networks using Genetic Algorithms 13\n(a)moons ,R= 50\n (b)circles ,R= 50Arch. GA GA (adaptive AB) Backprop.moonsA 90.7% \u00b17.2 90.9% \u00b19.1 99.4% \u00b12.4\nB 96.9% \u00b17.3 97.5% \u00b13.3 99.6% \u00b11.9\nC 98.4% \u00b12.5 98.9% \u00b11.7 99.8% \u00b11.4\nD 99.8% \u00b10.9 99.6% \u00b11.0 99.9% \u00b10.0circlesA 73.9% \u00b18.1 73.7% \u00b18.6 92.3% \u00b10.1\nB 87.3% \u00b13.6 86.8% \u00b14.4 92.4% \u00b10.0\nC 88.0% \u00b14.3 88.5% \u00b13.2 92.3% \u00b10.0\nD 91.6% \u00b10.5 88.3% \u00b14.0 92.4% \u00b10.0\n(c) Reached mean accuracies \u00b1stan-\ndard deviation\nFig. 3: Summary of the GA performance on the moons (a) and circles (b) datasets.\nThe blue boxplots represent multiple runs for each architecture using the default GA\nconfiguration, while the pink boxplots show the results of Rruns with the GA config-\nuration that incorporates an adaptive accuracy bound, initialized with a threshold value\nof0.85. For reference, we include the mean accuracies obtained from trained networks\nusing backpropagation. The achieved accuracies are detailed in (c). Figures and Table\ntaken from [1].\naccuracy bound, and there also seems to exist a minimum parameter count threshold\nbelow which final accuracies are significantly lower. Conversely, a similar positive ef-\nfect as the network size increases is less pronounced. Overall, the results suggest that\nthere are situations where the GA is able to achieve accuracies similar to backpropaga-\ntion.\nTo analyze the typical behavior of the GA in optimizing accuracy and sparsity, we ex-\namined a high-performing run from the experiments on the circles dataset. This run\nwas conducted using network architecture B and the GA configuration with an adap-\ntive accuracy bound. As shown in Fig. 4a, the fittest individual in the initial population\nachieved less than 65% accuracy. Over the course of the first 100 generations, accuracy\nimproved significantly, with rapid jumps, before stabilizing at around 91%, demonstrat-\ning the GA\u2019s optimization capabilities. After this point, until the termination condition\nis satisfied (i.e. reaching 200 generations), only marginal improvements occur.\nFig. 4b illustrates the evolution of sparsity throughout the same run. Initially, sparsity\ndecreases as accuracy is prioritized. Once accuracy gains slow down, the GA begins\noptimizing sparsity more effectively. At this stage, the population becomes highly ho-\nmogeneous, with many individuals achieving similar accuracy levels. Consequently, the\nGA shifts focus to sparsity, ultimately improving it by approximately 10% compared to\nthe best individual in the initial population. Note that this graph only shows the spar-\nsity development throughout the evolution and does not include the final sparsity level\nobtained after employing our post-evolutionary pruning routine.\nScalability The calculation of the fitness value for each individual in the population is\nthe critical factor in determining the runtime complexity of our GA. It can be expressed\nas the function O(g\u2217N\u2217(d\u2217l\u2217b2))describing the total number of multiplications\nperformed during evolution, with the number of generations g, the population size N,\nthe number of dataset samples dand the worst case network architecture characterized\nbyl\u2217b2parameters (that is, the length of the bit-vector). Typically, it holds that N < g\n\n--- Page 14 ---\n14 J. Sch\u00f6nberger et al.\n(a) Accuracy\n (b) Sparsity\nFig. 4: Optimization progress of a successful run using \u201cGA (adaptive AB)\u201d on network\narchitecture B = [2,75,2]for the circles dataset, tracking accuracy (a) and sparsity\n(b). The blue line represents the sparsity of the fittest individual in the current popula-\ntion, the orange line indicates the highest sparsity within the current generation, and the\ngreen line marks the best sparsity found across all previous generations. Figures taken\nfrom [1].\nand(l\u2217b2)\u226ad. In practice, the impact of population size and number of generations\non runtime can be mitigated by implementing efficient parallelization. Furthermore, a\ncompressed version of the subnetwork encoding can reduce the complexity of the other\nGA operations.\n5.2 Edge-Popup & Weight Initialization\nIn the preceding subsection, we demonstrated that the GA performs effectively on bi-\nnary classification tasks, achieving accuracies comparable to or even matching those ob-\ntained through backpropagation, provided an adequate network architecture is selected.\nTo assess how the GA compares to other methods for discovering SLTs in randomly ini-\ntialized neural networks, we repeat our experiments using the well-known edge-popup\nalgorithm [22].\nEdge-popup assigns a score to each network weight and extracts subnetworks by se-\nlecting the top k%scoring edges per layer for the forward pass. These scores are ad-\njusted during backpropagation using the straight-through gradient estimator [2]. Unlike\nstatic pruning, edge-popup allows previously removed edges to reappear, as their con-\ntributions to the loss are continuously reevaluated during gradient approximation. The\nparameter k, referred to as the pruning rate, defines the fraction of weights retained. For\ninstance, a 60% pruning rate results in a subnetwork where (1\u2212k) = 40% of weights\nare pruned. It is important to note that our sparsity metric is defined oppositely: a spar-\nsity of 60% corresponds to 60% of the weights being pruned.\nWe follow the authors\u2019 default settings and train for a total of 100epochs, evaluating\neach configuration across 25random seeds. Their experiments identified two partic-\nularly effective initialization methods: initializing network parameters from a Kaim-\ning normal distribution (also referred to as He initialization [8]), which we denote as\n\n--- Page 15 ---\nTowards Scalable Lottery Ticket Networks using Genetic Algorithms 15\n(a)moons, R= 25\n (b)circles, R= 25\nFig. 5: Visualization of edge-popup\u2019s performance across the displayed datasets, using\ndifferent color-coded initializations with Rruns each. For comparison, the mean accu-\nracies achieved through backpropagation on the respective architectures are shown as\ndashed lines. Figures taken from [1].\n\u201cWeights \u223c N k\u201d following Ramanujan et al. [22], and sampling from a signed Kaiming\nconstant distribution, referred to as \u201cWeights \u223cUk\u201d.\nIn addition to our own initialization method, labeled as \u201cWeights \u223c U [\u22121,1]\u201d we also\ninclude runs where networks are initialized using both of their approaches. Notably, we\nemploy the scaled versions of these methods, where the standard deviation is adjusted\nby a factor ofp\n1/k. For precise definitions, refer to Ramanujan et al. [22]. Analogously\nto our GA setup, when using our parameter initialization method with edge-popup, we\nsample biases from a uniform distribution.\nGiven the significant impact of parameter initialization on our GA\u2019s performance \u2014con-\nsistent with the findings of Ramanujan et al. [22]\u2014we begin with an ablation study to\nidentify the initialization technique that yields the highest accuracy for edge-popup be-\nfore conducting the main comparison experiments. Fig. 5a presents the results of edge-\npopup runs on the moons dataset. A clear trend emerges for the impact of the network\nsize on the final accuracies, akin to the one we observed for the GA: The larger net-\nworks generally achieve higher final accuracies on the moons dataset. Additionally,\napart from network A, runs using our initialization method consistently outperformed\nthe others. Notably, for network D, our method achieved a substantial improvement of\n\u22485%in mean accuracy compared to the other configurations. However, in all cases,\nedge-popup\u2019s mean accuracy remains noticeably lower than that of backpropagation.\nA similar pattern emerges in the circles experiment, as illustrated in Fig. 5b. How-\never, in this case, the Kaiming normal and signed Kaiming constant distributions proved\nentirely ineffective. None of the runs achieved classification accuracy beyond random\nguessing, only predicting the correct label in 50% of cases. At first glance, one might\nsuspect an issue with the edge-popup algorithm itself. However, since it performs well\nwith our initialization method, the problem most likely lies within the usage of the\nKaiming normal and signed Kaiming constant distributions. A possible explanation is\nthat the Gaussian nature of the rings in the dataset introduces distortions that negatively\nimpact these initialization methods. Determining the exact cause remains an open ques-\n\n--- Page 16 ---\n16 J. Sch\u00f6nberger et al.\ntion for future research. In summary, the results indicate that edge-popup and the GA\nboth benefit from the usage of a uniform initialization for the moons andcircles\ndatasets. We therefore decided to concentrate on the \u201cWeights \u223c U[\u22121,1]\u201d configuration\nfor the subsequent extensive comparative analysis.\nGiven the inverse relationship between accuracy and sparsity in the early stages of evo-\nlution (cf. Fig. 4b), we hypothesize a correlation between these factors. This suggests\nthat the number of parameters pruned throughout evolution may influence the final fit-\nness achieved. Unlike our approach, edge-popup keeps the number of pruned connec-\ntions in the subnetworks constant. To ensure that its performance is not hindered by this\nstrictness, we include additional edge-popup runs in our comparison study, where we\nset the pruning rates to match the mean sparsity levels achieved by the two GA con-\nfigurations. For each architecture and dataset, we selected the GA configuration that\nachieved the highest mean accuracy and reran the edge-popup experiments using the\ncorresponding mean sparsity levels.\nFig. 6 presents the results of our comparison study. To provide a comprehensive evalu-\nation, we plotted the mean accuracy of the best-performing GA configuration for each\narchitecture, alongside the mean accuracies of backpropagation and the original edge-\npopup runs. The shaded regions around the line plots indicate the 95% confidence in-\ntervals for the mean estimates. To facilitate the comparison of edge-popup with the\nadjusted pruning rates, we also display the mean sparsity levels achieved by our GA\nconfigurations (during the evolution) on the x-axis, below the network architectures. As\nshown in Fig. 6a, these sparsity levels decreased as network size increased, eventually\nconverging to 0.5, which coincides with edge-popup\u2019s default pruning rate. This sug-\ngests that the impact of varying the pruning rate should be more pronounced in smaller\narchitectures.\nIndeed, the most significant relative change is observed in network A. Here the adjusted\npruning rate led to several low-accuracy runs, lowering the overall mean. However, due\nto high variance, some instances still outperformed EP (50%) . For the larger networks,\nthe mean accuracy remained largely unchanged, and any observed differences were\nnegative. A similar pattern is observed for the circles dataset, as shown in Fig. 6b,\nwith the exception of architecture C. Given the substantial variance among runs with\nthe same pruning rate and the largely overlapping confidence intervals, it is unclear\nwhether these changes can be attributed to the adjusted pruning rates. Overall, none of\nthe modifications resulted in a notable performance improvement.\nWhen comparing edge-popup to the GA configurations, it is clear that the GA consis-\ntently outperforms edge-popup across all datasets and architectures, even when edge-\npopup is adapted to discover sparser subnetworks than in its original version. In fact,\nthe adjusted pruning rates result in worse performance. From this, we can conclude that\nthe GA is capable of finding subnetworks with higher accuracy that are also sparser,\nand that, for larger networks, it achieves performance comparable to backpropagation.\nTo evaluate the statistical significance of our findings, we apply a linear mixed model to\nthe accuracy data, particularly focusing on the comparison of various algorithms across\ndifferent architectures within a shared dataset. In doing so, we treated the algorithms as\nfixed effects, while we modeled the four architectures and different network initializa-\ntions as random effects, thereby aiming to capture variability across runs. We perform\n\n--- Page 17 ---\nTowards Scalable Lottery Ticket Networks using Genetic Algorithms 17\n(a)moons\n (b)circles\nDataset Reference Target Coef. Std. Err. z P > |z|95%-Conf.\nmoonsGA GA (adaptive AB) 0.042 0.083 0.507 0.612 [-0.121, 0.205]\nEP (50%) EP (adapted) -0.133 0.095 -1.405 0.160 [-0.320, 0.053]\nGA EP (50%) -1.185 0.081 -14.697 0.000 [-1.343, -1.027]\nGA Backpropagation 0.661 0.087 7.609 0.000 [0.491, 0.831]\ncirclesGA GA (adaptive AB) -0.106 0.064 -1.670 0.095 [-0.231, 0.018]\nEP (50%) EP (adapted) -0.062 0.102 -0.609 0.543 [-0.263, 0.138]\nGA EP (50%) -0.964 0.074 -13.095 0.000 [-1.109, -0.820]\nGA Backpropagation 1.030 0.071 14.595 0.000 [0.892, 1.168]\n(c) Statistical analysis\nFig. 6: Comparison of GA and edge popup performance across the displayed datasets\nusing the sparsity levels achieved by our GA configurations as new fixed pruning rates\nfor edge-popup. The selected mean sparsity levels are from either \u201cGA\u201d or \u201cGA (adap-\ntive AB)\u201d, as indicated by the different colored dots, depending on their respective mean\naccuracies. For reference, we show the mean accuracies and 95% confidence intervals\nfor the corresponding GA configuration, backpropagation, and the original edge popup\nvariant with the default pruning rate of 0.5. A final statistical analysis evaluates the per-\nformance differences between the algorithm combinations and provides p-values for the\nGA and edge popup configurations as well as for the backpropagation baseline. Figures\nand Table taken from [1].\n\n--- Page 18 ---\n18 J. Sch\u00f6nberger et al.\nour statistical analysis using the MixedLM module from [23]. In order to properly fit\nthe model and ensure convergence, we use Powell\u2019s algorithm , along with restricted\nmaximum likelihood (REML), and standardize the accuracy values. The results of this\nanalytical process are outlined in Table 6c.\nTo evaluate statistical significance, we analyze various statistics, including coefficients\nand p-values, to understand the relationship between the reference and target algo-\nrithms. For the moons dataset, the positive coefficient for GA (adaptive AB) suggests\na slight improvement over the standard GA across all architectures and initializations.\nHowever, since the p-value exceeds 0.05, this difference in performance is not statisti-\ncally significant. Similarly, for Edge Popup with the varied pruning rate, the negative\ncoefficient indicates slightly worse performance for EP (adapted) , which supports our\nprevious findings. Since the performances of both GA (adaptive AB) andEP (adapted)\nare not significantly different from the reference algorithms, we focus on comparing\nGAandEP (50%) . The comparison reveals a large negative coefficient, indicating that\nEP (50%) performs significantly worse. This difference is statistically significant with a\np-value of 0. When compared to backpropagation, the GA configuration shows a mod-\nerate decrease in performance, which is also statistically significant. For the circles\ndataset, the analysis presents a similar pattern. However, here, GA (adaptive AB) has a\nnegative coefficient, supporting the (almost statistically significant) result that the base\nGA configuration is more suitable for this dataset. Taking all random effects into ac-\ncount, backpropagation clearly outperforms the GA in this case.\nWe conclude this section with the general observation that the genetic algorithm has a\nsignificantly better performance than edge-popup in the considered settings. Addition-\nally, the GA algorithm demonstrates a performance that is only moderately worse than\nbackpropagation in terms of the final accuracy on the moons dataset.\n5.3 Multi-Class Performance\nSo far, we only considered datasets for binary classification using accuracy as perfor-\nmance metric. It turns out this approach has a much harder time finding suitable lottery\ntickets for multi-class classification problems. There are several reasons for that. A ma-\njor problem arises from the fact that the apparent correlation between a reduction in\nloss and an increase in accuracy, which is often observed in standard neural network\ntraining, does not generally apply in reverse. It is possible to obtain high accuracy net-\nworks that still have a high loss. A common loss function for classification tasks is the\ncross-entropy loss, which quantifies the model\u2019s uncertainty in assigning a sample to a\nspecific class. Let the model\u2019s output for a sample xiis denoted as \u02c6yi\u2208[0,1], which\ncorresponds to the predicted probability of yibeing the correct class label for xi. In\nthe best case, \u02c6yiis close to 1andyiis the correct label, resulting in a small loss. A\nmore problematic case occurs when the model still makes the correct prediction, but\nits predicted probability is close to random sampling (e.g. 0.5 in the binary case). We\nobserved that a lot of SLTs found by the GA fall into this second category, i.e. they\nachieve high accuracies but are very uncertain about their prediction. The uncertainty\nincreases as the number of classes increases. The consequences of this effect on the\nblobs dataset become apparent in Fig. 7a. While for 2\u20135 class instances, the GA can\n\n--- Page 19 ---\nTowards Scalable Lottery Ticket Networks using Genetic Algorithms 19\n(a) Accuracy, R= 25\n (b) Cross-Entropy Loss, R= 25\nFig. 7: Multi-Class performance on Rruns: (a) Overview of the distribution of final\naccuracies achieved by the GA (without accuracy bound) for different variants of the\nblobs dataset when using accuracy as objective function. For the binary case we con-\nsider class labels 0 and 1, for the ternary case 0, 1, and 2, and so on. All runs were\nperformed on architecture C (cf. Table 2e). (b) Performance of default GA configura-\ntion without accuracy bound on the blobs dataset optimizing the cross-entropy loss.\nFigure (a) taken from [1].\nretrieve subnetworks with perfect accuracy, trying to distinguish more class labels leads\nto increasingly bad final accuracies. Fortunately, this effect can be mitigated by opti-\nmizing the loss instead of the accuracy. Fig. 7b illustrates this change. We observe that\nthe accuracies of the previously problematic runs for 7\u201310 clusters are now elevated\nand, apart from the case with 10 classes, each setting includes runs that achieve approx-\nimately 100% test accuracy. A reason for the slight drop for the 10 classes case could\nbe the immediate proximity of the 9th and 10th cluster (the rightmost pink and olive\ncolored clusters in Fig. 2c), making their distinction considerably harder.\nTo better understand the difference in complexity that arises from optimizing different\nperformance metrics, we consider the shape of the respective objective functions. A\ndirect visualization of the optimization landscape of the GA is challenging because\nmasking is a discrete operation, and genetic operations are non-continuous. Instead,\nwe approximate its complexity by drawing the area of the continuous parameter space\naround a specific point (i.e. the parameters of the found SLT). For that, we adopt the\nvisualization technique used in [15] to obtain two-dimensional representations of the\nsurfaces of the high-dimensional cross-entropy loss function and the accuracy function.\nLetwsdenote the final parameter vector of a strong lottery ticket network discovered\nusing our genetic algorithm. Additionally, let d1andd2be two direction vectors, whose\nvalues are sampled from a Gaussian distribution with the same dimensionality as ws.\nUsing two scaling factors \u03b4, \u03b7\u2208Rwe can shift the original network to any point in the\nslice of the parameter space spanned by the transformation wt=f(ws, d1, d2, \u03b4, \u03b7) =\nws+\u03b4d1+\u03b7d2. For the transformed network, we can calculate the loss L(wt)and the\naccuracy A(wt). By selecting evenly spaced values for \u03b4and\u03b7, we can plot the SLT\u2019s\nloss and accuracy landscapes.\n\n--- Page 20 ---\n20 J. Sch\u00f6nberger et al.\n1.00\n 0.75\n 0.50\n 0.25\n 0.00 0.25 0.50 0.75 1.00\n1.00\n0.75\n0.50\n0.25\n0.000.250.500.751.00\n(a) Loss Landscape\n1.00\n 0.75\n 0.50\n 0.25\n 0.00 0.25 0.50 0.75 1.00\n1.00\n0.75\n0.50\n0.25\n0.000.250.500.751.00\n (b) Accuracy Landscape\nFig. 8: Visualization of the loss landscape (a) and the accuracy landscape (b) of a se-\nlected run from the multi-class classification experiment on the blobs dataset with 10\nclusters (cf. Fig. 7). \u03b4and\u03b7are our two scaling factors for the parameter transforma-\ntion. These plots only show a small portion of the landscapes. The color gradient from\npurple to orange indicates an improvement of the respective objective value.\nFig. 8 shows the results for one subnetwork obtained by executing the GA on the blobs\ndataset with 10 clusters. Fig. 8a visualizes the loss landscape for different \u03b4and\u03b7values\nfrom the range [\u22121,1]. Similar to contours in maps, the lines in this plot represent differ-\nent loss levels. The closer the lines are to one another, the steeper the ascent/descent. Re-\ngions with more distant contours correspond to relatively flat areas or plateaus. Darker\nlines represent larger loss values, brighter lines smaller ones. The original subnetwork\nis located in the middle of the loss landscape at point (0,0). It can be seen clearly, that\nthe SLT is located in a local minimum, with no other region exhibiting a smaller loss\nvalue. It is also apparent that the steepest ascent happens when \u03b4is close to 1.0. Overall\nthe displayed region seems to be relatively convex, promising an easier optimization,\nat least for this slice of the parameter space. This is not the case for the accuracy land-\nscape depicted in Fig. 8b. Here, brighter colors represent higher accuracies. Compared\nto the loss landscape, the accuracy landscape is extremely non-convex. The region with\nthe highest accuracy is very small and is concentrated on the closest neighbors of the\nSLT. This implies that small changes to the parameterization of the SLT already lead\nto a significant drop in accuracy. The large white space areas represent plateaus with\nonly small variability. All in all, the accuracy landscape appears to be very hostile, at\nleast for optimization algorithms that depend on local curvature information. From this\nanalysis, no final conclusion about the exact nature of the optimization landscapes can\nbe drawn, but it provides a general hint about the varying complexities the GA might\nencounter.\nBased on our previous results, we compare the performance of the GA, when opti-\nmizing the loss, with the performances achieved by edge-popup and networks trained\nvia backpropagation. We use the same dataset instance of the blobs dataset with 2\u2013\n\n--- Page 21 ---\nTowards Scalable Lottery Ticket Networks using Genetic Algorithms 21\n2 3 4 5 6 7 8 910\nNumber of Classes707580859095100 Accuracy (%)\nBackpropagation\nGA (normalized) GA EP (50%)\n(a)blobs ,R= 25No. Classes GA EP (50%) Backprop. GA (normalized)blobs2 100.0% \u00b1 0.0 100.0% \u00b1 0.0 100.0% \u00b1 0.0 100.0% \u00b1 0.0\n3 100.0% \u00b1 0.0 86.4% \u00b1 12.9 100.0% \u00b1 0.0 100.0% \u00b1 0.0\n4 99.97% \u00b1 0.2 92.4% \u00b1 8.6 100.0% \u00b1 0.0 100.0% \u00b1 0.0\n5 99.99% \u00b1 0.04 96.4% \u00b1 5.1 100.0% \u00b1 0.0 100.0% \u00b1 0.0\n6 95.5% \u00b1 9.2 94.9% \u00b1 7.2 100.0% \u00b1 0.0 100.0% \u00b1 0.0\n7 97.0% \u00b1 5.3 95.2% \u00b1 4.6 100.0% \u00b1 0.0 100.0% \u00b1 0.0\n8 97.4% \u00b1 4.8 94.7% \u00b1 4.9 100.0% \u00b1 0.0 100.0% \u00b1 0.0\n9 96.5% \u00b1 5.7 93.9% \u00b1 6.2 100.0% \u00b1 0.0 100.0% \u00b1 0.0\n10 89.6% \u00b1 7.8 83.4% \u00b1 7.4 99.9% \u00b1 0.3 99.6% \u00b1 2.0\n(b) Reached mean accuracies \u00b1 standard deviation\nFig. 9: (a) Performance comparison on the blobs dataset, based on R= 25 runs from\nthe standard GA configuration, edge-popup with 0.5 fixed prune-rate, backpropagation\nutilizing the hyperparameters from Table 2 and a GA evolved on the normalized ver-\nsion of the dataset. The markers correspond to mean accuracy values of the runs and\nthe vertical lines are the standard deviations. Top lines surpassing 100% accuracy are\ncropped. (b) A detailed numerical overview of the respective mean accuracies, includ-\ning the standard deviation.\n10 clusters and evaluate the algorithms on network architecture C.3For edge-popup,\nwe use the default version with 0.5 as fixed prune-rate. Apart from the general GA\nconfiguration (without the adaptive accuracy bound) we include runs obtained on a nor-\nmalized version of the dataset, which we will refer to as \u201cGA (normalized)\u201d. Fig. 9\ngives an overview of the results. The different shaped markers represent the mean ac-\ncuracies, while the length of the error bars indicate the standard deviation of the runs.\nNetworks trained with backpropagation achieve approximately 100% accuracy for all\ncluster numbers. The runs of the GAconfiguration (marked in orange) are the same as\nthose depicted in Fig. 7. Interestingly, the mean accuracy for 8 clusters is higher than for\n6 and 7 clusters, with a smaller standard deviation. A potential reason for this could be\nthat the runs with 6 and 7 clusters did not fully converge yet. Another possibility could\nbe that the relative positions of the additional clusters helped to better differentiate pre-\nviously hard to distinguish clusters. In comparison, apart from the scenario with only\ntwo clusters, edge-popup performs worse across all settings. Particularly noticeable is\nthe sudden drop for 3 clusters, with a subsequent recovery for clusters 4 and 5. There\nis a lot of performance variability between the runs, implying difficulties in proper con-\nvergence for different parameter initializations. Why this distinctive behavior occurred\nremains open for future work. For higher cluster numbers, the performance develop-\nment more closely resembles that of the GA, albeit still performing worse. A direct\ncomparison of \u201cGA (normalized)\u201d with \u201cGA\u201d and edge-popup would be unfair, but we\nincluded its data points to highlight the strong influence on the generalization perfor-\nmance of subnetworks found by the GA when the dataset is normalized. To explore the\n3It is likely, that using a larger architecture e.g. network D would result in slightly higher\naccuracy values, but for this experiment we focus on the relative performance and not on the\npotential absolute maximum.\n\n--- Page 22 ---\n22 J. Sch\u00f6nberger et al.\n2 3 4 5 10\nNumber of Classes707580859095100 Accuracy (%)\nBackpropagation\nGA (not-normalized) GA EP (50%)\n(a)digits ,R= 25No. Classes GA EP (50%) Backprop. GA (not-normalized)digits2 99.9% \u00b1 0.2 99.9% \u00b1 0.3 100.0% \u00b1 0.0 99.2% \u00b1 1.3\n3 99.7% \u00b1 0.4 99.7% \u00b1 0.4 100.0% \u00b1 0.0 96.0% \u00b1 3.3\n4 98.9% \u00b1 0.7 99.1% \u00b1 0.6 99.1% \u00b1 0.3 92.7% \u00b1 3.5\n5 98.6% \u00b1 0.8 99.4% \u00b1 0.5 100.0% \u00b1 0.0 91.0% \u00b1 2.4\n10 94.0% \u00b1 0.9 97.0% \u00b1 0.5 98.1% \u00b1 0.3 83.7% \u00b1 2.5\n(b) Reached mean accuracies \u00b1 standard deviation\nFig. 10: (a) Comparison of performance on the digits dataset, based on R= 25\nruns using the standard GA configuration, edge-popup with a fixed prune-rate of 0.5,\nbackpropagation with hyperparameters from Table 2, and a GA applied to the non-\nnormalized dataset. The markers represent the mean accuracy across the runs, while the\nvertical lines show the standard deviations. (b) A detailed numerical summary of the\nmean accuracies and their corresponding standard deviations.\nreason behind this, we computed the Hessian of the loss functions across different\nruns on both normalized and non-normalized datasets to assess the general curvature\nof the loss landscape. One observation is that compared to \u201cGA (normalized)\u201d, runs on\nthe non-normalized dataset tend to have a few very large eigenvalues, indicating that\nthe loss landscape around the minimum is very sharp in some directions. According to\ntheory, this could lead to a worse generalization [5].\nDigits Dataset Throughout our experiments on the blobs dataset, we uncovered sev-\neral important factors that exert a considerable influence on the efficacy of the GA\nwhen the number of clusters to differentiate increases, i.e. using a loss function as per-\nformance metric instead of directly optimizing the accuracy to reinforce the confidence\nin predicting class probabilities; and working on a normalized version of the dataset\nto reduce the risk of \u201csharp minima\u201d which could potentially lead to better general-\nization. In the remainder of this section, we want to study whether the GA can find\nhigh-performing subnetworks when, in addition to a higher number of class labels, the\ninput dimensionality is increased as well. For this purpose, we consider the digits\ndataset with up to 10 class labels ranging from {0, ...,9}and an input dimensionality\nof 64 pixel values. To adhere to the preliminary results of [1] we evaluate the GA and\nthe baselines on architecture B, using 2, 3, 4, 5, and 10 class labels respectively. This\nresults in bit masks containing 5550 bits (for the ten-classes case) instead of the pre-\nvious maximum of 400bits for the blobs dataset. We again use edge-popup with a\nfixed prune-rate of 0.5 and backpropagation-trained networks as baselines. In addition,\nwe also include results for the GA on the non-normalized dataset. Fig. 10 depicts our\nfindings.\nThe worst performing configuration is \u201cGA (not normalized)\u201d, which supports our as-\nsumption regarding the importance of normalization for datasets with similar properties\nto those considered here. Overall, the achieved mean accuracies are considerably higher\n\n--- Page 23 ---\nTowards Scalable Lottery Ticket Networks using Genetic Algorithms 23\n2 3 4 5 10\nNo. Classes020406080100 Mean Sparsity (%)52.8 51.3 52.0 52.2 51.798.094.090.486.5\n77.0Sparsity After Evolution\nSparsity After Post-Evolutionary Pruning\nFig. 11: Illustration of the capabilities of our post-evolutionary pruning routine applied\nto the best performing subnetworks found by the GA on the digits dataset. The\nresults are taken from the runs from the previous experiment (cf. Fig. 10). Displayed are\nthe mean sparsities of the subnetworks before and after applying the post-evolutionary\npruning routine. The numbers over the bars correspond to these mean sparsity values.\nfor the other algorithms compared to the blobs dataset. Noticeably, this experiment\nmarks the first instance where edge-popup scored higher accuracies compared to our\nGA, with a maximum performance difference of around 3% for the ten-classes case.\nThis could indicate that our current genetic operators are prone to suboptimal local\nminima for high parameter counts. Determining the exact reasons and exploring the\npotential of more sophisticated operators remains content of future work. Finally, we\nobserve a similar phenomenon to the one encountered for the blobs dataset where\nthere is a minor decline in accuracy in the four-classes case, with higher mean accura-\ncies achieved for the five-class setting, affecting both edge-popup and backpropagation,\nbut not our GA.\nThe default implementation of the generation operation of the GA (utilized in the pre-\nceding experiments) generates individuals with 50% sparsity when constructing the\ninitial population. For smaller network architectures, this initial sparsity is often in-\ncreased significantly during the evolution (cf. architecture A, Fig. 6a). The larger the\narchitecture, the smaller this relative improvement is. For architecture D on the moons\ndataset, the increase is only about 5%. As motivated in Section 3, applying a simple\npost-processing routine to remove as many remaining irrelevant weights as possible af-\nter the GA has terminated can additionally reduce the number of non-zero weights in the\nfound subnetwork. The enhanced capabilities of our pruning pipeline become evident\nwhen we consider high parameter networks like those used for the digits dataset.\nFig. 11 shows the sparsity development before and after applying the post-processing\nroutine on the evolved subnetworks. The blue bars in the figure demonstrate that, in the\ncourse of evolution, the initial sparsity is increased by less than 3% for all considered\nclass settings. On the other hand, the potential maximum sparsity (where accuracy is\n\n--- Page 24 ---\n24 J. Sch\u00f6nberger et al.\nnot yet affected) can be much higher: Up to 98% for the two-classes case, indicating an\nextreme overparameterization of the network architecture used. The achieved maximum\nsparsity after applying post-evolutional pruning decreases with additional class labels,\nbut still leads to an improvement of \u224825.3%for the ten-classes case. Given that a se-\nquential traversal of the final pruning mask has a runtime complexity of O(l\u2217b2), with\nl\u2217b2representing the potential worst-case size of the pruning mask (cf. Section 5.1),\nusing this additional routine does not affect the overall complexity of our algorithm,\nwhile adding substantial gains.\n6 Conclusion\nIn this work, we proposed a genetic algorithm for finding strong lottery ticket networks\nfollowing a new paradigm for optimizing network parameters that leads to high accu-\nracies without any conventional training steps. We have shown that the GA surpasses\nthe performance of the gradient-based state-of-the-art on three of the four considered\ndatasets and reaches comparable accuracy levels to networks trained with backprop-\nagation. Setting a minimum accuracy threshold via an adaptive accuracy limit when\ngenerating new individuals may lead to slight improvements, but these additional gains\nwere found to be statistically insignificant, at least for the datasets considered. Further-\nmore, our experiments highlight the importance of a proper weight initialization, when\nusing the GA and edge-popup. Although, optimizing accuracy directly did prove effec-\ntive for smaller architectures on the binary classification tasks, the highly non-convex\nshape of the accuracy landscape observable for larger architectures with several class\nlabels resulted in a significant drop in performance. Optimizing the cross-entropy loss\ninstead, and normalizing the datasets, lead to final accuracies matching those of back-\npropagation for the blobs dataset even for high cluster numbers. The digits dataset\nproved to be more challenging for the GA, with slightly lower performance levels than\nthe two baselines when the number of classes is high. Finally, we have accounted for\nthe prioritization of the performance objective during the evolution, resulting in com-\nparably smaller sparsity gains, by employing a \u201cpost-evolutionary pruning\u201d routine that\nsignificantly increases the sparsity of the final best performing subnetwork.\nApart from expanding our general understanding of the SLTH phenomenon and its\nimplications, there are several opportunities for future research. Considering the impor-\ntance of proper weight-initialization, it would be a promising research direction to study\ndifferent parameter initializations in the context of the SLTH, both from an empirical\nand a theoretical viewpoint. Much of the complexity of the optimization is the result\nof the extremely large search space. If we could restrict our search to a promising area\nof the search space, e.g. by considering only a subset of the model parameters while\nkeeping the others untouched, this would result in a much smaller number of candidate\nsubnetworks, likely leading to faster convergence. It is to be determined if such promis-\ning areas can be identified at the start of the evolution. Additionally, it might not be\nnecessary to consider all the training data for identifying high-performing subnetworks\n(cf. [33]). Much of the runtime in practice stems from evaluating individuals. Espe-\ncially for large-scale network architectures like transformer models, this can become\nproblematic when the dataset size is large. Apart from genetic algorithms, any method\n\n--- Page 25 ---\nTowards Scalable Lottery Ticket Networks using Genetic Algorithms 25\nsuitable for binary combinatorial optimization can in principle be applied to find strong\nlottery tickets. There might be other promising alternatives (cf. [29]).\nIt is important to note that, given the absence of gradient information in the GA, the\npotential applications of our approach extend beyond the realm of classical neural net-\nworks, which depend on differentiable functions. We considered accuracy and the cross-\nentropy loss for our performance evaluation, but using our GA it should be possible to\ndirectly incorporate non-differentiable evaluation metrics, such as edit distance [14] for\nstring comparisons or logical consistency checks in propositional logic \u2014 eliminating\nthe need for potentially suboptimal differentiable surrogates (cf. [16, 19]). This could\nhave significant implications for areas like natural language processing and neural rea-\nsoning. Finally, we only considered simple feed-forward neural networks and relatively\nsimple artificial classification tasks in this work. In a next step, we want to test the ca-\npabilities of the GA when applied to more complex network architectures and more\ndemanding learning tasks.\nOur work marks an important step toward more efficient and effective network architec-\ntures by explicitly leveraging a fundamental property of overparameterized networks.\nWe hypothesize that, beyond the existence of strong lottery tickets, other intrinsic char-\nacteristics of the parameter space can be exploited to further optimize model training.\nWe argue that the role of subnetworks should be reconsidered as fundamental build-\ning blocks of efficient deep learning models, attributing for much of the generalization\ncapabilities, requiring less computational resources, and enhancing scalability.\nAcknowledgments. This work was partially funded by the Bavarian Ministry for Economic Af-\nfairs, Regional Development and Energy as part of a project to support the thematic development\nof the Institute for Cognitive Systems.\nDisclosure of Interests. The authors declare that there are no conflicts of interest related to\nthis research. No financial, personal, or professional relationships have influenced the findings or\nconclusions presented in this work.\n\n--- Page 26 ---\nBibliography\n[1] Altmann, P., Sch\u00f6nberger, J., Zorn, M., Gabor, T.: Finding strong lottery ticket networks\nwith genetic algorithms. In: Proceedings of the 16th International Joint Conference on\nComputational Intelligence - V olume 1: NCTA. pp. 449\u2013460. INSTICC, SciTePress (2024).\nhttps://doi.org/10.5220/0013010300003837\n[2] Bengio, Y ., L\u00e9onard, N., Courville, A.: Estimating or propagating gradients through\nstochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432 (2013)\n[3] Chen, X., Zhang, J., Wang, Z.: Peek-a-boo: What (more) is disguised in a randomly\nweighted neural network, and how to find it efficiently. In: International Conference on\nLearning Representations (2021)\n[4] Deng, L.: The mnist database of handwritten digit images for machine learning research\n[best of the web]. IEEE Signal Processing Magazine 29(6), 141\u2013142 (2012). https://\ndoi.org/10.1109/MSP.2012.2211477\n[5] Dinh, L., Pascanu, R., Bengio, S., Bengio, Y .: Sharp minima can generalize for deep nets.\nIn: International Conference on Machine Learning. pp. 1019\u20131028. PMLR (2017)\n[6] Frankle, J., Carbin, M.: The lottery ticket hypothesis: Finding sparse, trainable neural net-\nworks. arXiv preprint arXiv:1803.03635 (2018)\n[7] Gaier, A., Ha, D.: Weight agnostic neural networks. Advances in neural information pro-\ncessing systems 32(2019)\n[8] He, K., Zhang, X., Ren, S., Sun, J.: Delving deep into rectifiers: Surpassing human-level\nperformance on imagenet classification. In: Proceedings of the IEEE international confer-\nence on computer vision. pp. 1026\u20131034 (2015)\n[9] Hinton, G., Vinyals, O., Dean, J.: Distilling the knowledge in a neural network. arXiv\npreprint arXiv:1503.02531 (2015)\n[10] Huang, G.B., Zhu, Q.Y ., Siew, C.K.: Extreme learning machine: theory and applications.\nNeurocomputing 70(1-3), 489\u2013501 (2006)\n[11] Jackson, A., Schoots, N., Ahantab, A., Luck, M., Black, E.: Finding sparse initialisations\nusing neuroevolutionary ticket search (nets). In: Artificial Life Conference Proceedings\n35. vol. 2023, p. 110. MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA\njournals-info . . . (2023)\n[12] Kasun, L.L.C., Zhou, H., Huang, G.B., V ong, C.M.: Representational learning with elms\nfor big data. IEEE Intelligent Systems (2013)\n[13] Lee, N., Ajanthan, T., Torr, P.H.: Snip: Single-shot network pruning based on connection\nsensitivity. arXiv preprint arXiv:1810.02340 (2018)\n[14] Levenshtein, V .I., et al.: Binary codes capable of correcting deletions, insertions, and rever-\nsals. In: Soviet physics doklady. vol. 10, pp. 707\u2013710. Soviet Union (1966)\n[15] Li, H., Xu, Z., Taylor, G., Studer, C., Goldstein, T.: Visualizing the loss landscape of neural\nnets. Advances in neural information processing systems 31(2018)\n[16] Li, T., Srikumar, V .: Augmenting neural networks with first-order logic. In: Proceedings of\nthe 57th Annual Meeting of the Association for Computational Linguistics (2019)\n[17] Malach, E., Yehudai, G., Shalev-Schwartz, S., Shamir, O.: Proving the lottery ticket hy-\npothesis: Pruning is all you need. In: International Conference on Machine Learning. pp.\n6682\u20136691. PMLR (2020)\n[18] Orseau, L., Hutter, M., Rivasplata, O.: Logarithmic pruning is all you need. Advances in\nNeural Information Processing Systems 33, 2925\u20132934 (2020)\n[19] Patel, Y ., Matas, J.: Feds-filtered edit distance surrogate. In: International Conference on\nDocument Analysis and Recognition. pp. 171\u2013186. Springer (2021)\n\n--- Page 27 ---\nTowards Scalable Lottery Ticket Networks using Genetic Algorithms 27\n[20] Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V ., Thirion, B., Grisel, O., Blondel,\nM., Prettenhofer, P., Weiss, R., Dubourg, V ., et al.: Scikit-learn: Machine learning in python.\nJournal of machine learning research 12(Oct), 2825\u20132830 (2011)\n[21] Pensia, A., Rajput, S., Nagle, A., Vishwakarma, H., Papailiopoulos, D.: Optimal lottery\ntickets via subset sum: Logarithmic over-parameterization is sufficient. Advances in neural\ninformation processing systems 33, 2599\u20132610 (2020)\n[22] Ramanujan, V ., Wortsman, M., Kembhavi, A., Farhadi, A., Rastegari, M.: What\u2019s hidden\nin a randomly weighted neural network? In: Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition. pp. 11893\u201311902 (2020)\n[23] Seabold, S., Perktold, J.: statsmodels: Econometric and statistical modeling with python.\nIn: 9th Python in Science Conference (2010)\n[24] Tanaka, H., Kunin, D., Yamins, D.L., Ganguli, S.: Pruning neural networks without any data\nby iteratively conserving synaptic flow. Advances in neural information processing systems\n33, 6377\u20136389 (2020)\n[25] Wang, C., Zhang, G., Grosse, R.: Picking winning tickets before training by preserving\ngradient flow. arXiv preprint arXiv:2002.07376 (2020)\n[26] Wang, H., Qin, C., Bai, Y ., Zhang, Y ., Fu, Y .: Recent advances on neural network pruning\nat initialization. arXiv preprint arXiv:2103.06460 (2021)\n[27] Wang, Y ., Zhang, X., Xie, L., Zhou, J., Su, H., Zhang, B., Hu, X.: Pruning from scratch. In:\nProceedings of the AAAI Conference on Artificial Intelligence. vol. 34, pp. 12273\u201312280\n(2020)\n[28] Wang, Z., Luo, T., Li, M., Zhou, J.T., Goh, R.S.M., Zhen, L.: Evolutionary multi-objective\nmodel compression for deep neural networks. IEEE Computational Intelligence Magazine\n16(3), 10\u201321 (2021)\n[29] Whitaker, T.: Quantum neuron selection: finding high performing subnetworks with quan-\ntum algorithms. In: Proceedings of the Genetic and Evolutionary Computation Conference\nCompanion. pp. 2258\u20132264 (2022)\n[30] Whitley, D., Tin\u00f3s, R., Chicano, F.: Optimal neuron selection: Nk echo state networks for\nreinforcement learning. arXiv preprint arXiv:1505.01887 (2015)\n[31] Wortsman, M., Farhadi, A., Rastegari, M.: Discovering neural wirings. Advances in Neural\nInformation Processing Systems 32(2019)\n[32] Wu, T., Li, X., Zhou, D., Li, N., Shi, J.: Differential evolution based layer-wise weight\npruning for compressing deep neural networks. Sensors 21(3), 880 (2021)\n[33] Zhang, Z., Chen, X., Chen, T., Wang, Z.: Efficient lottery ticket finding: Less data is more.\nIn: International Conference on Machine Learning. pp. 12380\u201312390. PMLR (2021)\n[34] Zhou, H., Lan, J., Liu, R., Yosinski, J.: Deconstructing lottery tickets: Zeros, signs, and the\nsupermask. Advances in neural information processing systems 32(2019)",
  "project_dir": "artifacts/projects/enhanced_cs.NE_2508.08877v1_Towards_Scalable_Lottery_Ticket_Networks_using_Gen",
  "communication_dir": "artifacts/projects/enhanced_cs.NE_2508.08877v1_Towards_Scalable_Lottery_Ticket_Networks_using_Gen/.agent_comm",
  "assigned_at": "2025-08-13T20:51:11.777244",
  "status": "assigned"
}